#######################################################################################
#   Complete pipeline for creating the MetaSRA database from the latest version of 
#   the SRAdb. Uses Condor to distribute the work across machines.
#######################################################################################

# Rules that should not run on cluster backends
localrules: all, download_SRAdb, compute_chunks, gather_term_mappings, build_final_json_db, build_final_sqlite_db

####### Pipeline parameters. Set these variables appropriately.

# This variable is used as the ID of the pipeline run. I usually use the date, but it
# can be any string.
VERSION = '2025-06-02'

# Where to save presistent output
PRESISTENT_DIR = f'/ua/ml-group/metadata-standardization-automation/metaSRA-build/{VERSION}'

# Download location of the SRA DB
SRADB_PATH = '/ua/ml-group/metadata-standardization-automation/metaSRA-data/SRAmetadb_2022-08-26.sqlite'

# The absolute path of the repo, accessible to condor (i.e. on a shared drive)
METASRA_REPO = '/ua/ml-group/metadata-standardization-automation/MetaSRA-pipeline'

# Location to place the SRA metadata files 
SRA_DB_DESTINATION = f'{PRESISTENT_DIR}/sra_db'

# Location to place the input files for Condor that are shared across jobs
CONDOR_INPUT_LOC = f'{PRESISTENT_DIR}/condor_input'

# Location of the script to create the raw metadata JSON file that is fed to each Condor job
#EXTRACT_METADATA_JSON_LOCATION = f'{METASRA_REPO}/create_metasra'

# Location of map_sra_to_ontology
PIPELINE_SRC_LOC = f'{METASRA_REPO}/map_sra_to_ontology'

# Location of pipeline_v53.py
#BUILD_PIPELINE_LOC = f'{METASRA_REPO}/all_pipelines'

# Location of all the scripts for running the MetaSRA pipeline (not the MetaSRA code itself)
CREATE_METASRA_SRC = f'{METASRA_REPO}/create_metasra'

# Location of the Condor work directory
CONDOR_WORK_LOC = f'{PRESISTENT_DIR}/condor_work'

# Location of the output files
OUTPUT_LOC = f'{PRESISTENT_DIR}/create_metasra_output'


####### These variables can be left alone.

# Conda environment to use for running steps
CONDA_ENV = '../environment.yml'

# Location of SRAdb
SRA_DB_LOCATION = 'https://gbnci.cancer.gov/sra/SRAmetadb.sqlite.gz'

# Name of the file output by each Condor job
PER_JOB_OUTPUT_FILENAME = 'metasra_mappings.json'

# Name of the Condor submit file
SUBMIT_FILENAME = 'create_metasra.submit'

# Name of the file that Condor has terminated
FINISH_FILENAME = 'finished.txt'

# The name of the file storing the raw output from all Condor jobs
RAW_MAPPINGS_FILENAME = 'metasra_raw_mappings.{}.json'.format(VERSION)

# The name of the file storing the predicted sample types
SAMPLE_TYPE_PREDICTIONS_FILENAME = 'sample_type_predictions.{}.json'.format(VERSION)

# One rule to rule them all, one rule to bind them...
rule all:
    input:
        '{output_loc}/metasra.RNA_Seq.human.{today}.json'.format(
            output_loc=OUTPUT_LOC,
            today=VERSION
        ),
        '{output_loc}/metasra.RNA_Seq.human.{today}.sqlite'.format(
            output_loc=OUTPUT_LOC,
            today=VERSION
        ),
        '{output_loc}/metasra.ChIP_Seq.human.{today}.json'.format(
            output_loc=OUTPUT_LOC,
            today=VERSION
        ),
        '{output_loc}/metasra.ChIP_Seq.human.{today}.sqlite'.format(
            output_loc=OUTPUT_LOC,
            today=VERSION
        ),
        '{output_loc}/metasra.RNA_Seq.mouse.{today}.json'.format(
            output_loc=OUTPUT_LOC,
            today=VERSION
        ),
        '{output_loc}/metasra.RNA_Seq.mouse.{today}.sqlite'.format(
            output_loc=OUTPUT_LOC,
            today=VERSION
        ),
        '{output_loc}/metasra.ChIP_Seq.mouse.{today}.json'.format(
            output_loc=OUTPUT_LOC,
            today=VERSION
        ),
        '{output_loc}/metasra.ChIP_Seq.mouse.{today}.sqlite'.format(
            output_loc=OUTPUT_LOC,
            today=VERSION
        )

##################################################################################
# Download the SRAdb. This is the database storing the raw
# metadata for the entire SRA. See the following:
# https://bioconductor.org/packages/release/bioc/html/SRAdb.html
##################################################################################
rule download_SRAdb:
    output:
        '{sra_db_dest}/SRAmetadb.{date}.sqlite'.format(
            sra_db_dest=SRA_DB_DESTINATION,
            date=VERSION
        )
    run:
        commands=[
            "curl {sra_db_loc} > {{output}}.gz".format(
                sra_db_loc=SRA_DB_LOCATION
            ),
            "gunzip -f {output}.gz"
        ]
        for c in commands:
            shell('echo "{}"'.format(c))
            shell(c)


##################################################################################
# We only want a subset of the metadata, so I create a new SQLite
# file storing only the data we want to standardize. Furthermore, 
# this parses the key-value pairs in the SRAdb and stores them into
# their own table in this 'subset' database.
##################################################################################
rule build_custom_SRAdb:
    input:
        SRADB_PATH
    output:
        f'{SRA_DB_DESTINATION}/SRAmetadb.subdb.{VERSION}.{{assay}}.{{species}}.sqlite'
    shell:
        'python build_subdb.py {wildcards.assay} {wildcards.species} -t {input} -s {output}'


##################################################################################
# We extract the key-value pairs in the 'subset' database into a
# JSON file, which is provided as input to all of the downstream
# Condor jobs. 
##################################################################################
rule extract_json_from_sqlite:
    input:
        f'{SRA_DB_DESTINATION}/SRAmetadb.subdb.{VERSION}.{{assay}}.{{species}}.sqlite'
    output:
        f'{SRA_DB_DESTINATION}/sample_to_raw_metadata.{{assay}}.{{species}}.json'
    conda:
        CONDA_ENV
    shell:
        (
            f'PYTHONPATH={METASRA_REPO} '
            'python extract_raw_metadata_json.py {input} {output}'
        )

##################################################################################
# Prepare chunks for parallel execution
##################################################################################

checkpoint compute_chunks:
    input:
        f'{SRA_DB_DESTINATION}/sample_to_raw_metadata.{{assay}}.{{species}}.json'
    output:
        directory(f'{CONDOR_INPUT_LOC}/sample_chunks.{{assay}}.{{species}}')
    run:
        import json
        from pathlib import Path

        with open(input[0], 'rt') as instream:
            all_samples = list(json.load(instream).keys())
        
        out_dir = Path(output[0])
        out_dir.mkdir(parents=True, exist_ok=True)

        chunk_size = 500

        for i in range((len(all_samples) // chunk_size)+1):

            chunk = all_samples[i*chunk_size:(i+1)*chunk_size]

            with (out_dir / f'chunk_{i}.json').open('wt') as outstream:
                json.dump(
                    {
                        'sample_accessions': list(chunk)
                    },
                    outstream
                )

##################################################################################
# Run term mapping job
##################################################################################

rule run_term_mapping:
    input:
        samples_list=f'{CONDOR_INPUT_LOC}/sample_chunks.{{assay}}.{{species}}/chunk_{{chunk}}.json',
        sample_to_raw_metadata=f'{SRA_DB_DESTINATION}/sample_to_raw_metadata.{{assay}}.{{species}}.json'
    output:
        f'{CONDOR_WORK_LOC}/{{assay}}.{{species}}.{{chunk}}/{FINISH_FILENAME}'
    conda:
        CONDA_ENV
    shell:
        #f"""
        #cd {METASRA_REPO}
        #source init-environment.sh
        #cd create_metasra
        f"""
        PYTHONPATH={METASRA_REPO} \
        python condor_run_pipeline.py \
            -s {{input.samples_list}} \
            -m {{input.sample_to_raw_metadata}} \
            -o {{output}}
        """


##################################################################################
# Gather term mapping outputs
##################################################################################

def gather_term_mappings_input(wildcards):
    chunks_dir = checkpoints.compute_chunks.get(**wildcards).output[0]
    return expand(
        f'{CONDOR_WORK_LOC}/{{assay}}.{{species}}.{{chunk}}/{FINISH_FILENAME}',
        chunk=glob_wildcards(f'{chunks_dir}/chunk_{{chunk}}.json').chunk,
        **wildcards
    )


rule gather_term_mappings:
    input:
        gather_term_mappings_input
    output:
        f'{OUTPUT_LOC}/{{assay}}.{{species}}/{RAW_MAPPINGS_FILENAME}'
    run:
        import json

        def read_json(f):
            try:
                with open(f, 'rt') as instream:
                    return json.load(instream)
            except:
                print(f'Failed to read {f}')
                raise

        with open(output[0], 'wt') as outstream:
            json.dump(
                {
                    k: v
                    for f in input
                    for k, v in read_json(f).items()
                },
                outstream
            )


##################################################################################
# Predict the sample-type for all samples. The file 
# <OUTPUT_LOC>/predict_sample_type.log stores information about this step.
##################################################################################
rule predict_sample_type:
    input:
        metadata='{}/sample_to_raw_metadata.{{assay}}.{{species}}.json'.format(
            SRA_DB_DESTINATION
        ),
        mappings='{condor_output}/{{assay}}.{{species}}/{raw_mappings_f}'.format(
            condor_output=OUTPUT_LOC,
            raw_mappings_f=RAW_MAPPINGS_FILENAME
        )
    output:
        result='{condor_output}/{{assay}}.{{species}}/{predictions_f}'.format(
            condor_output=OUTPUT_LOC,
            predictions_f=SAMPLE_TYPE_PREDICTIONS_FILENAME
        ),
        log='{}/{{assay}}.{{species}}/predict_sample_type.log'.format(
            OUTPUT_LOC
        )
    conda:
        CONDA_ENV
    shell:
        (
            f'python {PIPELINE_SRC_LOC}/predict_sample_type/run_on_entire_dataset.py'
            ' {input.metadata} {input.mappings} {output.result} {output.log}'
        )

##################################################################################
# Build the final database files.
##################################################################################
rule build_database_files:
    input:
        mappings='{condor_output}/{{assay}}.{{species}}/{raw_mappings_f}'.format(
            condor_output=OUTPUT_LOC,
            raw_mappings_f=RAW_MAPPINGS_FILENAME
        ),
        predictions='{condor_output}/{{assay}}.{{species}}/{predictions_f}'.format(
            condor_output=OUTPUT_LOC,
            predictions_f=SAMPLE_TYPE_PREDICTIONS_FILENAME
        )
    output:
        json_f='{output_loc}/metasra.{{assay}}.{{species}}.{today}.json'.format(
            output_loc=OUTPUT_LOC,
            today=VERSION
        ),
        sql_f='{output_loc}/metasra.{{assay}}.{{species}}.{today}.sqlite'.format(
            output_loc=OUTPUT_LOC,
            today=VERSION
        )
    conda:
        CONDA_ENV
    shell:
        (
            f'python {CREATE_METASRA_SRC}/build_metasra_database_files.py'
            ' {input.mappings} {input.predictions} {output.json_f} {output.sql_f}'
        )

##################################################################################
# Join the various species and assays files into one JSON and one Sqlite file.
###################################################################################
rule build_final_json_db:
    input:
        human_rna='{output_loc}/metasra.RNA_Seq.human.{today}.json'.format(
            output_loc=OUTPUT_LOC,
            today=VERSION
        ),
        human_chip='{output_loc}/metasra.ChIP_Seq.human.{today}.json'.format(
            output_loc=OUTPUT_LOC,
            today=VERSION
        ),
        mouse_rna='{output_loc}/metasra.RNA_Seq.mouse.{today}.json'.format(
            output_loc=OUTPUT_LOC,
            today=VERSION
        ),
        mouse_chip='{output_loc}/metasra.ChIP_Seq.mouse.{today}.json'.format(
            output_loc=OUTPUT_LOC,
            today=VERSION
        )
    output:
        '{output_loc}/metasra.final.{today}.json'.format(
            output_loc=OUTPUT_LOC,
            today=VERSION
        )
    conda:
        CONDA_ENV
    shell:
        (
            'python join_assay_json_dbs.py '
            '{input.human_rna},'
            '{input.human_chip},'
            '{input.mouse_rna},'
            '{input.mouse_chip} '
            'RNA-seq,ChIP-seq,RNA-seq,ChIP-seq '
            ' human,human,mouse,mouse -o {output}'            
        )
        
rule build_final_sqlite_db:
    input:
        human_rna='{output_loc}/metasra.RNA_Seq.human.{today}.sqlite'.format(
            output_loc=OUTPUT_LOC,
            today=VERSION
        ),
        human_chip='{output_loc}/metasra.ChIP_Seq.human.{today}.sqlite'.format(
            output_loc=OUTPUT_LOC,
            today=VERSION
        ),
        mouse_rna='{output_loc}/metasra.RNA_Seq.mouse.{today}.sqlite'.format(
            output_loc=OUTPUT_LOC,
            today=VERSION
        ),
        mouse_chip='{output_loc}/metasra.ChIP_Seq.mouse.{today}.sqlite'.format(
            output_loc=OUTPUT_LOC,
            today=VERSION
        )
    conda:
        CONDA_ENV
    output:
        '{output_loc}/metasra.final.{today}.sqlite'.format(
            output_loc=OUTPUT_LOC,
            today=VERSION
        )
    shell:
        (
            'python join_assay_sqlite_dbs.py '
            '{input.human_rna},'
            '{input.human_chip},'
            '{input.mouse_rna},'
            '{input.mouse_chip} '
            'RNA-seq,ChIP-seq,RNA-seq,ChIP-seq '
            'human,human,mouse,mouse '
            '-o {output}'
        )
